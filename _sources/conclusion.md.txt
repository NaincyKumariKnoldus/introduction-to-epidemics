# Conclusion

```{epigraph}

Many are fleeing, everyone is fearful, you are neither â€“ splendid,
magnificent! For what is more foolish than to fear what you cannot
avoid by any strategy, and what you aggravate by fearing? What is more
useless than to flee what will always confront you wherever you may
flee?

-- Petrarch, *Letters of Old Age*.
```

What, then, can we conclude from this brief and superficial look at
epidemic modelling on networks? I would like to think that there are
several broad take-away messages.

The most important message by far is that -- despite using advanced
mathematics, detailed sets of parameters, and extensive computer
simulation -- modelling remains an inexact science. It's important to
qualify that word "inexact": while models and simulations can generate
results in extraordinary volume and with great precision, the
*interpretation* of those results inevitably involves judgement
calls. Many details remain unknown, and in many cases unknowable,
perhaps because they cannot be properly measured, or perhaps because
they change so fast that measurement is quickly outdated by
events. Whatever the reason, no model *in itself* tells us anything;
rather, they provide evidence to guide our thinking.

A corollary to this is that the policy responses to an epidemic can
only partially be driven by, determined by, or justified by, the
results of modelling. Policy remains an essentially political
activity, and while it may be "driven by" or "informed by" science,
there will always be other factors needing to be included that may
skew a final decision away from what a scientist may view as
"correct". Many real-world problems are
[*wicked*](https://en.wikipedia.org/wiki/Wicked_problem), impossible
to solve because of inherent contradictions and the compromises they
imply, but mandating an immediate response nonetheless.

In many ways this makes modelling *more* important, not less. A model
provides only a limited view onto any problem. But the fact that it
can provide a view onto *any* problem means that we can explore
problems we haven't yet faced, explore techniques we couldn't yet
deploy in reality, and so forth. It is at least important to
understand things that *can't* happen as it is to understand those
that *can*, if only to cut down the space of possibilities that need
further consideration.

The second take-away message is the scientific underpinnings of many
policies with which we're familiar -- so much so that they sometimes
feel almost part of the world's folklore. Vaccination, quarantine,
physical isolation, herd immunity, and so forth are all susceptible to
exploration and variation. And the science can expose commonalities
that are not initially obvious: that the provision of protective
equipment behaves like vaccination, for example, in the way it can be
used to reduce the dangers of super-spreading. This can lead to
alternative approaches.

The third message concerns countermeasures. We saw when we discussed
{ref}`adaptive countermeasures <sec:seir-adaptive-seir>` that
variations in the efficacy with which the processes were carried out
made a huge difference to the results. In the real world, of course,
one may be *stuck* with ineffective processes: an imprecise test, a
limited number of testers-and-tracers, and so forth. This may defeat
even a well-thought-through strategy.

The implication of this is that any countermeasure is an *experiment*
-- and the same is true of any attempt to unwind a countermeasure,
such as for example when coming our of a physical distancing
lockdown. It's possible that the strategy will fail, and that measures
will need to be re-imposed. This may be difficult for people to take,
especially if they've not be warned of the possibility beforehand.

The final message is the most important for me as an academic: the
*democracy* of science. People sometimes feel that science is something
alien, requiring endless qualifications, state or corporate
sponsorship, and access to techniques and tools that are out of reach
of the amateur. Nothing could be further from the truth.

Science, as practiced by real scientists, is largely just an exercise
of their curiosity that's conducted within a framework -- the
*scientific method* -- that has evolved over the years to help stop us
misleading ourselves. The framework isn't a barrier to entry into
science; rather, it's a guide to help identify simple truth within a
complex reality. 

The {ref}`quotation from Rovelli <sec:model-expectations>` with which
we opened this book highlights that the conclusions drawn by science
are always tentative and open to question, refutation, and
overthrow. It's working within this framework that makes a practice
into science, not the letters after the practitioner's name. And while
we hope that well-qualified people are right sufficiently often to be
trusted, that's an authority that has to be earned and justified by a
willingness to accept correction as part of the process of
truth-finding.

If this book shows anything, I hope it's that computational science is
within the reach of everyone. It's not the preserve of academics,
although academic scientists have developed many of the ideas and
tools; it doesn't need supercomputers, although they're often useful;
and no-one should be afraid of posing any question: any question,
sincerely asked, is worth asking, and worth working to find an answer
to.




